{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4362,
     "status": "ok",
     "timestamp": 1694691355947,
     "user": {
      "displayName": "Sebastian Moguilner",
      "userId": "13806978311057858956"
     },
     "user_tz": 240
    },
    "id": "4hx4ae9iDhq7",
    "outputId": "8032141e-b3f1-4714-cf3d-07ec3bf5923b"
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import LeaveOneOut, GridSearchCV\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 169,
     "status": "ok",
     "timestamp": 1694691560440,
     "user": {
      "displayName": "Sebastian Moguilner",
      "userId": "13806978311057858956"
     },
     "user_tz": 240
    },
    "id": "UCc5CN_mmfm6"
   },
   "outputs": [],
   "source": [
    "def ml_classifier(input_table, target, n_iterations, classifier_type='xgb'):\n",
    "    \"\"\"\n",
    "    Train a classifier (XGBoost, Lasso, ElasticNet, Ridge) using Leave-One-Out cross-validation and obtain test-set results\n",
    "    via bootstrap.\n",
    "\n",
    "    Args:\n",
    "        input_table (pd.DataFrame): Input dataset containing features and class labels.\n",
    "        target (string): Column to predict.\n",
    "        n_iterations (int): Number of bootstrap iterations to perform.\n",
    "        classifier_type (string): Type of classifier to use ('xgb', 'lasso', 'elasticnet', 'ridge').\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the AUC, FPR, TPR, thresholds, and feature importances for each iteration.\n",
    "\n",
    "    Classifier Details:\n",
    "        'xgb': XGBoost is a gradient boosting algorithm.\n",
    "               Advantages: Handles missing values, non-linear data, and performs well in many scenarios.\n",
    "               Disadvantages: Might overfit on noisy data, requires careful tuning.\n",
    "\n",
    "        'lasso': Lasso classification uses Logistic Regression with L1 penalty.\n",
    "                 Advantages: Performs feature selection by pushing less important features' coefficients to zero.\n",
    "                 Disadvantages: Can't select more features than samples.\n",
    "\n",
    "        'elasticnet': ElasticNet classification uses a combination of L1 and L2 penalties.\n",
    "                      Advantages: Can learn a sparse model where few features are influential, and can select more features than samples.\n",
    "                      Disadvantages: Requires setting two hyperparameters.\n",
    "\n",
    "        'ridge': Ridge classification uses Logistic Regression with L2 penalty.\n",
    "                 Advantages: Prevents multicollinearity in regression model.\n",
    "                 Disadvantages: Doesn't perform feature selection like Lasso.\n",
    "\n",
    "    \"\"\"\n",
    "    # Bootstrap 90% of the sample size each time\n",
    "    np.random.seed(42)\n",
    "    n_size = int(len(input_table) * 0.9)\n",
    "\n",
    "    # Initialize variables\n",
    "    stats = list()\n",
    "    metrics = ['auc', 'fpr', 'tpr', 'thresholds', 'feature_importances']\n",
    "    results = {'main': {m: [] for m in metrics}}\n",
    "\n",
    "    # Define hyperparameter grid for C\n",
    "    param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "    \n",
    "    for i in tqdm(range(n_iterations), desc=\"Bootstrap iterations\"):\n",
    "        subsampled_data = resample(input_table, n_samples=n_size, stratify=input_table[target].values)\n",
    "        y = subsampled_data[target].values\n",
    "        X = subsampled_data.drop(columns=[target]).values\n",
    "\n",
    "        # Data normalization\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "\n",
    "        loo = LeaveOneOut()\n",
    "        loo.get_n_splits(X)\n",
    "\n",
    "        labels = []\n",
    "        probabilities = []\n",
    "        coefficients = []\n",
    "\n",
    "        for train_index, test_index in loo.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            if classifier_type == 'xgb':\n",
    "                clf = xgb.XGBClassifier(eval_metric='logloss', use_label_encoder=False)\n",
    "                clf.fit(X_train, y_train)\n",
    "                coefficients.append(clf.feature_importances_)\n",
    "\n",
    "            elif classifier_type in ['lasso', 'elasticnet', 'ridge']:\n",
    "                if classifier_type == 'lasso':\n",
    "                    base_clf = LogisticRegression(penalty='l1', solver='liblinear', class_weight='balanced', max_iter=10000, tol=1e-2)\n",
    "                if classifier_type == 'elasticnet':\n",
    "                    base_clf = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, class_weight='balanced', max_iter=10000, tol=1e-2)\n",
    "                elif classifier_type == 'ridge':\n",
    "                    base_clf = LogisticRegression(penalty='l2', solver='sag', class_weight='balanced', max_iter=10000, tol=1e-2)\n",
    "\n",
    "                # Apply grid search on the base classifier\n",
    "                grid_clf = GridSearchCV(base_clf, param_grid, cv=3)  # Using 3-fold CV for grid search\n",
    "                grid_clf.fit(X_train, y_train)\n",
    "                clf = grid_clf.best_estimator_  # Get the best classifier\n",
    "                coefficients.append(np.abs(clf.coef_[0]))\n",
    "\n",
    "            labels.append(y_test)\n",
    "            probabilities.append(clf.predict_proba(X_test)[:, 1])\n",
    "\n",
    "        stats.append(roc_auc_score(labels, probabilities))\n",
    "        fpr, tpr, thresholds = roc_curve(labels, probabilities)\n",
    "        results['main']['fpr'].append(fpr)\n",
    "        results['main']['tpr'].append(tpr)\n",
    "        results['main']['thresholds'].append(thresholds)\n",
    "        results['main']['auc'].append(roc_auc_score(labels, probabilities))\n",
    "        results['main']['feature_importances'].append(np.mean(coefficients, axis=0))\n",
    "\n",
    "    return results\n",
    "\n",
    "def plot_roc_all_features(results, n_iterations):\n",
    "    \"\"\"\n",
    "    This function creates and plots a ROC for a classifier's performance using all features.\n",
    "\n",
    "    Args:\n",
    "    results (dict): A dictionary containing the results of the classifier, including 'fpr', 'tpr', and 'auc' keys.\n",
    "    n_iterations (int): The number of iterations used for creating the interpolated TPR values.\n",
    "\n",
    "    Returns:\n",
    "    None: The function saves the ROC plot as an image file and displays it.\n",
    "    \"\"\"\n",
    "    # Set plot parameters\n",
    "    colors = {\n",
    "        'filla': 'rgba(52, 152, 219, 0.2)',\n",
    "        'linea': 'rgba(52, 152, 219, 0.5)',\n",
    "        'maina': 'rgba(41, 128, 185, 1.0)',\n",
    "        'grid': 'rgba(189, 195, 199, 0.5)',\n",
    "        'annot': 'rgba(149, 165, 166, 0.5)',\n",
    "        'highlight': 'rgba(192, 57, 43, 1.0)'\n",
    "    }\n",
    "\n",
    "    fpr_mean = np.linspace(0, 1, n_iterations)\n",
    "    interp_tprs = []\n",
    "\n",
    "    # Calculate confidence bands\n",
    "    for i in range(n_iterations):\n",
    "        fpr, tpr = results['main']['fpr'][i], results['main']['tpr'][i]\n",
    "        interp_tprs.append(np.interp(fpr_mean, fpr, tpr))\n",
    "        interp_tprs[-1][0] = 0.0\n",
    "\n",
    "    tpr_mean = np.mean(interp_tprs, axis=0)\n",
    "    tpr_mean[-1] = 1.0\n",
    "\n",
    "    tpr_ci = np.std(interp_tprs, axis=0) * 1.96\n",
    "    tpr_upper = np.clip(tpr_mean + tpr_ci, 0, 1)\n",
    "    tpr_lower = tpr_mean - tpr_ci\n",
    "\n",
    "    auc = np.mean(results['main']['auc'])\n",
    "\n",
    "    plot_data = [\n",
    "        go.Scatter(x=fpr_mean, y=tpr_upper, line=dict(color=colors['linea'], width=1), hoverinfo=\"skip\", showlegend=False, name='upper'),\n",
    "        go.Scatter(x=fpr_mean, y=tpr_lower, fill='tonexty', fillcolor=colors['filla'], line=dict(color=colors['linea'], width=1), hoverinfo=\"skip\", showlegend=False, name='lower'),\n",
    "        go.Scatter(x=fpr_mean, y=tpr_mean, line=dict(color=colors['maina'], width=2), hoverinfo=\"skip\", showlegend=True, name=f'AUC = {auc:.3f} [{tpr_lower.mean():.3f} {tpr_upper.mean():.3f}]')\n",
    "    ]\n",
    "\n",
    "    fig = go.Figure(plot_data)\n",
    "    fig.add_shape(type='line', line=dict(dash='dash'), x0=0, x1=1, y0=0, y1=1)\n",
    "\n",
    "    fig.update_layout(\n",
    "        template='plotly_white',\n",
    "        title_x=0.5,\n",
    "        xaxis_title=\"1 - Specificity\",\n",
    "        yaxis_title=\"Sensitivity\",\n",
    "        width=600,\n",
    "        height=600,\n",
    "        legend=dict(yanchor=\"bottom\", xanchor=\"right\", x=0.95, y=0.01),\n",
    "        font=dict(family=\"Arial\", size=22, color=\"black\")\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(range=[0, 1], gridcolor=colors['grid'], scaleanchor=\"x\", scaleratio=1, linecolor='black')\n",
    "    fig.update_xaxes(range=[0, 1], gridcolor=colors['grid'], constrain='domain', linecolor='black')\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "def plot_feature_importances(results, input_table, target):\n",
    "    \"\"\"\n",
    "    Plot the feature importances based on the output of ml_classifier using a horizontal bar plot.\n",
    "\n",
    "    Args:\n",
    "        results (dict): The results dictionary from ml_classifier.\n",
    "        input_table (pd.DataFrame): Input dataset.\n",
    "        target (string): Target column.\n",
    "\n",
    "    Returns:\n",
    "        None: Shows the feature importance plot.\n",
    "    \"\"\"\n",
    "\n",
    "    # Correctly access 'feature_importances' from the nested dictionary structure\n",
    "    feature_importance_mean = np.mean(results['main']['feature_importances'], axis=0)\n",
    "    feature_importance_std = np.std(results['main']['feature_importances'], axis=0)\n",
    "\n",
    "    # Calculate SEM\n",
    "    n = len(results['main']['feature_importances'])\n",
    "    sem = feature_importance_std / np.sqrt(n)\n",
    "\n",
    "    # Get feature names\n",
    "    feature_names = input_table.drop(columns=[target]).columns.tolist()\n",
    "\n",
    "    # Match feature name convention from the group classifier\n",
    "    def rearrange_name(name):\n",
    "        parts = name.split()\n",
    "        type_ = parts[1]   # 'wPLI', 'AEC-c'\n",
    "        feature = parts[2] # 'N2-delta', 'N2-theta'\n",
    "        return f\"{feature}-{type_}\"\n",
    "\n",
    "    # Rearrange names\n",
    "    feature_names = [rearrange_name(name) for name in feature_names]\n",
    "    \n",
    "    # Sorting the features by importance in ascending order\n",
    "    sorted_idx = feature_importance_mean.argsort()\n",
    "    feature_importance_mean = feature_importance_mean[sorted_idx]\n",
    "    sem = sem[sorted_idx]\n",
    "    feature_names_sorted = [feature_names[i] for i in sorted_idx]\n",
    "    \n",
    "    # Select the top ten features\n",
    "    top_n = 10\n",
    "    feature_importance_mean = feature_importance_mean[-top_n:]\n",
    "    sem = sem[-top_n:]\n",
    "    feature_names_sorted = feature_names_sorted[-top_n:]\n",
    "\n",
    "    # Create a horizontal bar plot\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # We use y-axis for feature names and x-axis for feature importance values in horizontal bar plots\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            y=feature_names_sorted,\n",
    "            x=feature_importance_mean,\n",
    "            orientation='h',\n",
    "            error_x=dict(type='data', array=sem, visible=True)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Layout configuration\n",
    "    fig.update_layout(\n",
    "        # title=\"Feature Importances with SEM\",\n",
    "        xaxis_title=\"Feature Importance\",\n",
    "        yaxis_title=\"GE features\",\n",
    "        template='plotly_white',\n",
    "        width=600,\n",
    "        height=600,\n",
    "        font=dict(family=\"Arial\", size=22, color=\"black\")\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1694691356219,
     "user": {
      "displayName": "Sebastian Moguilner",
      "userId": "13806978311057858956"
     },
     "user_tz": 240
    },
    "id": "zgw9NPdekW6V",
    "outputId": "e5c8b64b-f555-485d-dd91-cfe0f3cd2d06",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Load MoCA decline data\n",
    "decline_df = pd.read_excel('./MoCA_change_2.xlsx')\n",
    "decline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1694691356220,
     "user": {
      "displayName": "Sebastian Moguilner",
      "userId": "13806978311057858956"
     },
     "user_tz": 240
    },
    "id": "oVy_FRj4lk7F",
    "outputId": "8477af86-381c-4589-a5a0-1d41a8fa94af"
   },
   "outputs": [],
   "source": [
    "# Remove ID duplicates and unnecessary columns\n",
    "decline_df = decline_df.drop_duplicates(subset='ID', keep='first')\n",
    "decline_df = decline_df.drop(columns = ['Sbj_ID_Date', 'Categorization',\t'MoCA',\t'MoCA_DurationFollowup_Years',\t'MoCA_Time_Coded_FirstLast',\t'MoCA_Score_FirstVisit',\t'MoCA_Score_LastVisit',\t'MoCA_AnnualChange'], axis = 1)\n",
    "decline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of subjects to analyze\n",
    "len(decline_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1694691356221,
     "user": {
      "displayName": "Sebastian Moguilner",
      "userId": "13806978311057858956"
     },
     "user_tz": 240
    },
    "id": "h9Padx-xHD2Y",
    "outputId": "1a9c3eee-116a-499e-9b60-3615d765e65c"
   },
   "outputs": [],
   "source": [
    "## Load all Global Efficiency features\n",
    "all_features = pd.read_csv('./all_GE_AEC_wPLI_features.csv')\n",
    "## Remove subjects not having all the stages (NaNs)\n",
    "all_features = all_features.dropna()\n",
    "all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "executionInfo": {
     "elapsed": 203,
     "status": "ok",
     "timestamp": 1694691356404,
     "user": {
      "displayName": "Sebastian Moguilner",
      "userId": "13806978311057858956"
     },
     "user_tz": 240
    },
    "id": "YshlWg1QpRvO",
    "outputId": "75577911-215e-4693-ba18-1e9e53d0c0a7"
   },
   "outputs": [],
   "source": [
    "## Merge the MoCA decline tertiles with the GE features\n",
    "df = pd.merge(decline_df, all_features, on = 'ID')\n",
    "df = df.drop(columns = ['ID', 'Class'], axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1694691356404,
     "user": {
      "displayName": "Sebastian Moguilner",
      "userId": "13806978311057858956"
     },
     "user_tz": 240
    },
    "id": "PKkRwV6qp5h8",
    "outputId": "a762820f-4228-4496-d3db-4e447daab38c"
   },
   "outputs": [],
   "source": [
    "# Leave most affected class (1st tertile) as class number 1, and change the rest to class 0\n",
    "df['MoCA_AnnualChange_Tertile'] = df['MoCA_AnnualChange_Tertile'].replace({2: 0, 3: 0})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 417128,
     "status": "ok",
     "timestamp": 1694691983321,
     "user": {
      "displayName": "Sebastian Moguilner",
      "userId": "13806978311057858956"
     },
     "user_tz": 240
    },
    "id": "Ui_4omCmC8mk",
    "outputId": "2f365c24-f3f1-4f66-b273-ec9ffb7e738e"
   },
   "outputs": [],
   "source": [
    "# Run the XGBoost classifier\n",
    "# Number of iterations for bootstrap\n",
    "n_iterations = 1000\n",
    "\n",
    "results = ml_classifier(df, \"MoCA_AnnualChange_Tertile\", n_iterations, classifier_type='xgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "executionInfo": {
     "elapsed": 569,
     "status": "ok",
     "timestamp": 1694691983866,
     "user": {
      "displayName": "Sebastian Moguilner",
      "userId": "13806978311057858956"
     },
     "user_tz": 240
    },
    "id": "xjNInErjFcVj",
    "outputId": "4b789dae-0feb-4098-df7d-a4a4511e99b5"
   },
   "outputs": [],
   "source": [
    "plot_roc_all_features(results, n_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1694691983866,
     "user": {
      "displayName": "Sebastian Moguilner",
      "userId": "13806978311057858956"
     },
     "user_tz": 240
    },
    "id": "AW00ktABMw1F",
    "outputId": "16021447-f6a1-4e92-9dfc-00d64d4ca16e"
   },
   "outputs": [],
   "source": [
    "plot_feature_importances(results, df, \"MoCA_AnnualChange_Tertile\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN+eaAecDkBrmPHZABxdnEH",
   "provenance": [
    {
     "file_id": "12se4CgCQ8v9wl4z3aFfcolM39oOVqm9Z",
     "timestamp": 1692025295325
    },
    {
     "file_id": "14E95VOJyfI2kT75a95DPO60kUVLXBb72",
     "timestamp": 1691087757506
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
