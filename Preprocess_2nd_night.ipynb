{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 81727,
     "status": "ok",
     "timestamp": 1684488600394,
     "user": {
      "displayName": "Sebastian Moguilner",
      "userId": "13806978311057858956"
     },
     "user_tz": 240
    },
    "id": "_7Vt73f62o58",
    "outputId": "5aef68f8-988e-4dc8-cbb0-57a60e067ba3"
   },
   "outputs": [],
   "source": [
    "from scipy.signal import welch, csd\n",
    "import itertools\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from statannot import add_stat_annotation\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "import seaborn as sns; sns.set_theme(color_codes=True)\n",
    "from scipy.signal import butter, lfilter\n",
    "import scipy.io\n",
    "import glob\n",
    "from scipy.stats import normaltest\n",
    "\n",
    "import mne\n",
    "import mne_connectivity\n",
    "from mne_connectivity import envelope_correlation\n",
    "from mne_connectivity import spectral_connectivity_epochs\n",
    "\n",
    "from scipy.stats import pearsonr,spearmanr\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os, stat\n",
    "\n",
    "from scipy.signal import find_peaks\n",
    "import math\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.integrate import simps\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.utils import resample\n",
    "import scipy.io as sio\n",
    "from pprint import pprint\n",
    "import matplotlib.colors as mcolors\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1684488600396,
     "user": {
      "displayName": "Sebastian Moguilner",
      "userId": "13806978311057858956"
     },
     "user_tz": 240
    },
    "id": "vendvuwRyw9y"
   },
   "outputs": [],
   "source": [
    "## BCTPy functions\n",
    "## https://github.com/aestrivex/bctpy\n",
    "\n",
    "def efficiency_wei(Gw, local=False):\n",
    "    '''\n",
    "    The global efficiency is the average of inverse shortest path length,\n",
    "    and is inversely related to the characteristic path length.\n",
    "    The local efficiency is the global efficiency computed on the\n",
    "    neighborhood of the node, and is related to the clustering coefficient.\n",
    "    Parameters\n",
    "    ----------\n",
    "    W : NxN np.ndarray\n",
    "        undirected weighted connection matrix\n",
    "        (all weights in W must be between 0 and 1)\n",
    "    local = bool or enum\n",
    "        If True or 'local', computes local efficiency instead of global efficiency.\n",
    "        If False or 'global', uses the global efficiency\n",
    "        If 'original', will use the original algorithm provided by (Rubinov\n",
    "        & Sporns 2010). This version is not recommended. The local efficiency\n",
    "        calculation was improved in (Wang et al. 2016) as a true generalization\n",
    "        of the binary variant.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Eglob : float\n",
    "        global efficiency, only if local in (False, 'global')\n",
    "    Eloc : Nx1 np.ndarray\n",
    "        local efficiency, only if local in (True, 'local', 'original')\n",
    "    Notes\n",
    "    -----\n",
    "       The  efficiency is computed using an auxiliary connection-length\n",
    "    matrix L, defined as L_ij = 1/W_ij for all nonzero L_ij; This has an\n",
    "    intuitive interpretation, as higher connection weights intuitively\n",
    "    correspond to shorter lengths.\n",
    "       The weighted local efficiency broadly parallels the weighted\n",
    "    clustering coefficient of Onnela et al. (2005) and distinguishes the\n",
    "    influence of different paths based on connection weights of the\n",
    "    corresponding neighbors to the node in question. In other words, a path\n",
    "    between two neighbors with strong connections to the node in question\n",
    "    contributes more to the local efficiency than a path between two weakly\n",
    "    connected neighbors. Note that this weighted variant of the local\n",
    "    efficiency is hence not a strict generalization of the binary variant.\n",
    "    Algorithm:  Dijkstra's algorithm\n",
    "    '''\n",
    "    if local not in (True, False, 'local', 'global', 'original'):\n",
    "        raise BCTParamError(\"local param must be any of True, False, \"\n",
    "            \"'local', 'global', or 'original'\")\n",
    "\n",
    "    def distance_inv_wei(G):\n",
    "        n = len(G)\n",
    "        D = np.zeros((n, n))  # distance matrix\n",
    "        D[np.logical_not(np.eye(n))] = np.inf\n",
    "\n",
    "        for u in range(n):\n",
    "            # distance permanence (true is temporary)\n",
    "            S = np.ones((n,), dtype=bool)\n",
    "            G1 = G.copy()\n",
    "            V = [u]\n",
    "            while True:\n",
    "                S[V] = 0  # distance u->V is now permanent\n",
    "                G1[:, V] = 0  # no in-edges as already shortest\n",
    "                for v in V:\n",
    "                    W, = np.where(G1[v, :])  # neighbors of smallest nodes\n",
    "                    td = np.array(\n",
    "                        [D[u, W].flatten(), (D[u, v] + G1[v, W]).flatten()])\n",
    "                    D[u, W] = np.min(td, axis=0)\n",
    "\n",
    "                if D[u, S].size == 0:  # all nodes reached\n",
    "                    break\n",
    "                minD = np.min(D[u, S])\n",
    "                if np.isinf(minD):  # some nodes cannot be reached\n",
    "                    break\n",
    "                V, = np.where(D[u, :] == minD)\n",
    "\n",
    "        np.fill_diagonal(D, 1)\n",
    "        D = 1 / D\n",
    "        np.fill_diagonal(D, 0)\n",
    "        return D\n",
    "\n",
    "    n = len(Gw)\n",
    "    Gl = invert(Gw, copy=True)  # connection length matrix\n",
    "    A = np.array((Gw != 0), dtype=int)\n",
    "    #local efficiency algorithm described by Rubinov and Sporns 2010, not recommended\n",
    "    if local == 'original':\n",
    "        E = np.zeros((n,))\n",
    "        for u in range(n):\n",
    "            # V,=np.where(Gw[u,:])\t\t#neighbors\n",
    "            # k=len(V)\t\t\t\t\t#degree\n",
    "            # if k>=2:\t\t\t\t\t#degree must be at least 2\n",
    "            #\te=(distance_inv_wei(Gl[V].T[V])*np.outer(Gw[V,u],Gw[u,V]))**1/3\n",
    "            #\tE[u]=np.sum(e)/(k*k-k)\n",
    "\n",
    "            # find pairs of neighbors\n",
    "            V, = np.where(np.logical_or(Gw[u, :], Gw[:, u].T))\n",
    "            # symmetrized vector of weights\n",
    "            sw = cuberoot(Gw[u, V]) + cuberoot(Gw[V, u].T)\n",
    "            # inverse distance matrix\n",
    "            e = distance_inv_wei(Gl[np.ix_(V, V)])\n",
    "            # symmetrized inverse distance matrix\n",
    "            se = cuberoot(e) + cuberoot(e.T)\n",
    "\n",
    "            numer = np.sum(np.outer(sw.T, sw) * se) / 2\n",
    "            if numer != 0:\n",
    "                # symmetrized adjacency vector\n",
    "                sa = A[u, V] + A[V, u].T\n",
    "                denom = np.sum(sa)**2 - np.sum(sa * sa)\n",
    "                # print numer,denom\n",
    "                E[u] = numer / denom  # local efficiency\n",
    "\n",
    "    #local efficiency algorithm described by Wang et al 2016, recommended\n",
    "    elif local in (True, 'local'):\n",
    "        E = np.zeros((n,))\n",
    "        for u in range(n):\n",
    "            V, = np.where(np.logical_or(Gw[u, :], Gw[:, u].T))\n",
    "            sw = cuberoot(Gw[u, V]) + cuberoot(Gw[V, u].T)\n",
    "            e = distance_inv_wei(cuberoot(Gl)[np.ix_(V, V)])\n",
    "            se = e+e.T\n",
    "         \n",
    "            numer = np.sum(np.outer(sw.T, sw) * se) / 2\n",
    "            if numer != 0:\n",
    "                # symmetrized adjacency vector\n",
    "                sa = A[u, V] + A[V, u].T\n",
    "                denom = np.sum(sa)**2 - np.sum(sa * sa)\n",
    "                # print numer,denom\n",
    "                E[u] = numer / denom  # local efficiency\n",
    "\n",
    "    elif local in (False, 'global'):\n",
    "        e = distance_inv_wei(Gl)\n",
    "        E = np.sum(e) / (n * n - n)\n",
    "    return E\n",
    "\n",
    "def invert(W, copy=True):\n",
    "    '''\n",
    "    Inverts elementwise the weights in an input connection matrix.\n",
    "    In other words, change the from the matrix of internode strengths to the\n",
    "    matrix of internode distances.\n",
    "    If copy is not set, this function will *modify W in place.*\n",
    "    Parameters\n",
    "    ----------\n",
    "    W : np.ndarray\n",
    "        weighted connectivity matrix\n",
    "    copy : bool\n",
    "        if True, returns a copy of the matrix. Otherwise, modifies the matrix\n",
    "        in place. Default value=True.\n",
    "    Returns\n",
    "    -------\n",
    "    W : np.ndarray\n",
    "        inverted connectivity matrix\n",
    "    '''\n",
    "    if copy:\n",
    "        W = W.copy()\n",
    "    E = np.where(W)\n",
    "    W[E] = 1. / W[E]\n",
    "    return W\n",
    "\n",
    "def clustering_coef_wu(W):\n",
    "    '''\n",
    "    The weighted clustering coefficient is the average \"intensity\" of\n",
    "    triangles around a node.\n",
    "    Parameters\n",
    "    ----------\n",
    "    W : NxN np.ndarray\n",
    "        weighted undirected connection matrix\n",
    "    Returns\n",
    "    -------\n",
    "    C : Nx1 np.ndarray\n",
    "        clustering coefficient vector\n",
    "    '''\n",
    "    K = np.array(np.sum(np.logical_not(W == 0), axis=1), dtype=float)\n",
    "    ws = cuberoot(W)\n",
    "    cyc3 = np.diag(np.dot(ws, np.dot(ws, ws)))\n",
    "    K[np.where(cyc3 == 0)] = np.inf  # if no 3-cycles exist, set C=0\n",
    "    C = cyc3 / (K * (K - 1))\n",
    "    return C\n",
    "\n",
    "def distance_wei(G):\n",
    "    '''\n",
    "    The distance matrix contains lengths of shortest paths between all\n",
    "    pairs of nodes. An entry (u,v) represents the length of shortest path\n",
    "    from node u to node v. The average shortest path length is the\n",
    "    characteristic path length of the network.\n",
    "    Parameters\n",
    "    ----------\n",
    "    L : NxN np.ndarray\n",
    "        Directed/undirected connection-length matrix.\n",
    "        NB L is not the adjacency matrix. See below.\n",
    "    Returns\n",
    "    -------\n",
    "    D : NxN np.ndarray\n",
    "        distance (shortest weighted path) matrix\n",
    "    B : NxN np.ndarray\n",
    "        matrix of number of edges in shortest weighted path\n",
    "    Notes\n",
    "    -----\n",
    "       The input matrix must be a connection-length matrix, typically\n",
    "    obtained via a mapping from weight to length. For instance, in a\n",
    "    weighted correlation network higher correlations are more naturally\n",
    "    interpreted as shorter distances and the input matrix should\n",
    "    consequently be some inverse of the connectivity matrix.\n",
    "       The number of edges in shortest weighted paths may in general\n",
    "    exceed the number of edges in shortest binary paths (i.e. shortest\n",
    "    paths computed on the binarized connectivity matrix), because shortest\n",
    "    weighted paths have the minimal weighted distance, but not necessarily\n",
    "    the minimal number of edges.\n",
    "       Lengths between disconnected nodes are set to Inf.\n",
    "       Lengths on the main diagonal are set to 0.\n",
    "    Algorithm: Dijkstra's algorithm.\n",
    "    '''\n",
    "    n = len(G)\n",
    "    D = np.zeros((n, n))  # distance matrix\n",
    "    D[np.logical_not(np.eye(n))] = np.inf\n",
    "    B = np.zeros((n, n))  # number of edges matrix\n",
    "\n",
    "    for u in range(n):\n",
    "        # distance permanence (true is temporary)\n",
    "        S = np.ones((n,), dtype=bool)\n",
    "        G1 = G.copy()\n",
    "        V = [u]\n",
    "        while True:\n",
    "            S[V] = 0  # distance u->V is now permanent\n",
    "            G1[:, V] = 0  # no in-edges as already shortest\n",
    "            for v in V:\n",
    "                W, = np.where(G1[v, :])  # neighbors of shortest nodes\n",
    "\n",
    "                td = np.array(\n",
    "                    [D[u, W].flatten(), (D[u, v] + G1[v, W]).flatten()])\n",
    "                d = np.min(td, axis=0)\n",
    "                wi = np.argmin(td, axis=0)\n",
    "\n",
    "                D[u, W] = d  # smallest of old/new path lengths\n",
    "                ind = W[np.where(wi == 1)]  # indices of lengthened paths\n",
    "                # increment nr_edges for lengthened paths\n",
    "                B[u, ind] = B[u, v] + 1\n",
    "\n",
    "            if D[u, S].size == 0:  # all nodes reached\n",
    "                break\n",
    "            minD = np.min(D[u, S])\n",
    "            if np.isinf(minD):  # some nodes cannot be reached\n",
    "                break\n",
    "\n",
    "            V, = np.where(D[u, :] == minD)\n",
    "\n",
    "    return D, B\n",
    "\n",
    "def cuberoot(x):\n",
    "    '''\n",
    "    Correctly handle the cube root for negative weights, instead of uselessly\n",
    "    crashing as in python or returning the wrong root as in matlab\n",
    "    '''\n",
    "    return np.sign(x) * np.abs(x)**(1 / 3)\n",
    "\n",
    "def smallworldness_wei(mat):\n",
    "    '''\n",
    "    Small-Worldness is defined as the ratio of the average clustering coefficient to the average path length. \n",
    "    This function computes the small-worldness of a graph given its adjacency matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mat : numpy.ndarray\n",
    "        The adjacency matrix of the graph. A square, symmetric matrix where an entry (i, j) \n",
    "        represents the weight of the edge between nodes i and j.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The small-worldness of the graph.\n",
    "\n",
    "    '''\n",
    "    # Calculate the connection-length matrix by taking the element-wise reciprocal of the adjacency matrix\n",
    "    connection_length_matrix = np.divide(1, mat, out=np.zeros_like(mat), where=(mat != 0))\n",
    "\n",
    "    # Compute the average clustering coefficient\n",
    "    avg_clustering_coef = np.nanmean(clustering_coef_wu(mat))\n",
    "\n",
    "    # Compute the average shortest path length (characteristic path length)\n",
    "    avg_path_length = np.nanmean(distance_wei(connection_length_matrix)[0])\n",
    "\n",
    "    # Calculate the small-worldness\n",
    "    sw = avg_clustering_coef / avg_path_length\n",
    "\n",
    "    return sw\n",
    "\n",
    "class BCTParamError(RuntimeError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1684488600397,
     "user": {
      "displayName": "Sebastian Moguilner",
      "userId": "13806978311057858956"
     },
     "user_tz": 240
    },
    "id": "rwDgzi6awIhb"
   },
   "outputs": [],
   "source": [
    "def psd(signals, sf, fromf, tof):\n",
    "    '''\n",
    "    This function computes the power spectral density (PSD) of the input signals using the Welch method\n",
    "    and calculates the relative power in the specified frequency range.\n",
    "\n",
    "    Args:\n",
    "    signals (array): An array containing the input signals with shape (epochs, channels, samples).\n",
    "    sf (int): The sampling frequency of the input signals.\n",
    "    fromf (float): The lower limit of the frequency range of interest.\n",
    "    tof (float): The upper limit of the frequency range of interest.\n",
    "\n",
    "    Returns:\n",
    "    pow (float): The mean relative power of the input signals in the specified frequency range.\n",
    "    '''\n",
    "    \n",
    "    # In the context of the Welch method, a window size of 4 seconds is long enough to obtain a reasonable frequency \n",
    "    # resolution for the analysis of EEG rhythms (e.g., delta, theta, alpha, beta, and gamma bands),\n",
    "    # while still being short enough to capture the non-stationary nature of the EEG data.\n",
    "    \n",
    "    win = 4*sf\n",
    "    freqs, psd = welch(signals, sf, nperseg=win)\n",
    "\n",
    "    # Define delta lower and upper limits\n",
    "    low, high = fromf, tof\n",
    "\n",
    "    # Find intersecting values in frequency vector\n",
    "    idx_delta = np.logical_and(freqs >= low, freqs <= high)\n",
    "\n",
    "    # Frequency resolution\n",
    "    freq_res = freqs[1] - freqs[0]  # = 1 / 4 = 0.25 Hz minimum frequency to be captured\n",
    "    power = np.zeros((psd.shape[0],psd.shape[1]))\n",
    "\n",
    "    for i in range(psd.shape[0]):\n",
    "        for j in range(psd.shape[1]):\n",
    "            power[i,j] = simps(psd[i,j,:][idx_delta], dx=freq_res)/simps(psd[i,j,:], dx=freq_res)\n",
    "\n",
    "    pow = np.mean(power)\n",
    "\n",
    "    return pow\n",
    "\n",
    "def to_bipolar(x, channel_map):\n",
    "    \"\"\"\n",
    "    This function converts the input data to bipolar montage.\n",
    "\n",
    "    Args:\n",
    "    x (numpy.ndarray): The input data in a numpy array format.\n",
    "    channel_map (dict): A dictionary that maps the channel names to their corresponding indices.\n",
    "\n",
    "    Returns:\n",
    "    x_bp (numpy.ndarray): The bipolar montage data in a numpy array format.\n",
    "    channels_all (list): A list of channel names in the bipolar montage.\n",
    "    \"\"\"\n",
    "\n",
    "    channels_all = ['Fp1-Fp2','F7-Fp1','F8-Fp2','F7-F3','F8-F4',\n",
    "    'F3-Fz','F4-Fz','C3-Cz','C4-Cz','T3-C3','T4-C4','T5-P3',\n",
    "    'T6-P4','P3-Pz','P4-Pz', 'T5-O1','T6-O2','O1-O2']\n",
    "\n",
    "    n_channels = np.shape(channels_all)[0]\n",
    "    n_samples = np.shape(x)[1]\n",
    "\n",
    "    x_bp = np.zeros((n_channels, n_samples))\n",
    "\n",
    "    for i in range(n_channels):\n",
    "        channels_cr = str.split(channels_all[i], '-')\n",
    "        x_bp[i] = x[channel_map[channels_cr[0]]] - x[channel_map[channels_cr[1]]]\n",
    "        \n",
    "    return x_bp, channels_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1684488600399,
     "user": {
      "displayName": "Sebastian Moguilner",
      "userId": "13806978311057858956"
     },
     "user_tz": 240
    },
    "id": "iOsmmGjD2o6E"
   },
   "outputs": [],
   "source": [
    "# Set time series corresponding to each channel name\n",
    "CHANNEL_MAP = {'Fp1':0,'F7':1,'T3':2,'T5':3,'O1':4,'F3':5,'C3':6,\n",
    "               'P3':7,'Fz':8,'Cz':9,'Pz':10,'Fp2':11,'F8':12,'T4':13,\n",
    "               'T6':14,'O2':15,'F4':16,'C4':17,'P4':18,'T1':19,'T2':20,'EKG':21}\n",
    "\n",
    "FS = 200  # sampling frequency\n",
    "EPOCH_SIZE = 30.0  # size of an individual epoch\n",
    "EPOCH_SIZE_S = int(EPOCH_SIZE*FS)\n",
    "\n",
    "# Subjects excluded from the entire analysis\n",
    "excl_entire = ['ADEX_084'] # AD-NoEp\n",
    "\n",
    "# Subjects excluded from awake analysis only\n",
    "excl_awake = ['ADEX_104', # AD-Ep\n",
    "            'ADEX_026', # AD-NoEp \n",
    "            'ADEX_047', # HC\n",
    "            'ADEX_130'] # AD-NoEp \n",
    "             \n",
    "# Subjects to invert polarity\n",
    "invert_files = ['ADEX_019',\n",
    "                'ADEX_031',\n",
    "                'ADEX_055',\n",
    "                'ADEX_057',\n",
    "                'ADEX_060',\n",
    "                'ADEX_061',\n",
    "                'ADEX_065',\n",
    "                'ADEX_066',\n",
    "                'ADEX_067',\n",
    "                'ADEX_070',\n",
    "                'ADEX_071',\n",
    "                'ADEX_072',\n",
    "                'ADEX_076',\n",
    "                'ADEX_077',\n",
    "                'ADEX_081',\n",
    "                'ADEX_086',\n",
    "                'ADEX_087',\n",
    "                'ADEX_088',\n",
    "                'ADEX_092',\n",
    "                'ADEX_093',\n",
    "                'ADEX_097',\n",
    "                'ADEX_100',\n",
    "                'ADEX_104',\n",
    "                'ADEX_105',\n",
    "                'ADEX_110',\n",
    "                'ADEX_111']\n",
    "\n",
    "# Staging files having specific start times on each epoch\n",
    "stagingspecial    = ['ADEX_025',\n",
    "                     'ADEX_139',\n",
    "                     'ADEX_013',\n",
    "                     'ADEX_137',\n",
    "                     'ADEX_118',\n",
    "                     'ADEX_053',\n",
    "                     'ADEX_113',\n",
    "                     'ADEX_050',\n",
    "                     'ADEX_119',\n",
    "                     'ADEX_018',\n",
    "                     'ADEX_130',\n",
    "                     'ADEX_043',\n",
    "                     'ADEX_114',\n",
    "                     'ADEX_008',\n",
    "                     'ADEX_042',\n",
    "                     'ADEX_125',\n",
    "                     'ADEX_116',\n",
    "                     'ADEX_027',\n",
    "                     'ADEX_069',\n",
    "                     'ADEX_048',\n",
    "                     'ADEX_136',\n",
    "                     'ADEX_140',\n",
    "                     'ADEX_047',\n",
    "                     'ADEX_120',\n",
    "                     'ADEX_117',\n",
    "                     'ADEX_126',\n",
    "                     'ADEX_014',\n",
    "                     'ADEX_135',\n",
    "                     'ADEX_026',\n",
    "                     'ADEX_132',\n",
    "                     'ADEX_020',\n",
    "                     'ADEX_127',\n",
    "                     'ADEX_129',\n",
    "                     'ADEX_005',\n",
    "                     'ADEX_128',\n",
    "                     'ADEX_138',\n",
    "                     'ADEX_019',\n",
    "                     'ADEX_084',\n",
    "                     'ADEX_087',\n",
    "                     'ADEX_097',\n",
    "                     'ADEX_100']\n",
    "# Bipolar channels\n",
    "channels = ['Fp1-Fp2', 'F7-Fp1', 'F8-Fp2', 'F7-F3', 'F8-F4', 'F3-Fz', 'F4-Fz', 'C3-Cz',\n",
    "            'C4-Cz', 'T3-C3', 'T4-C4', 'T5-P3', 'T6-P4', 'P3-Pz', 'P4-Pz', 'T5-O1',\n",
    "            'T6-O2', 'O1-O2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, filename, stage, fromfreq, tofreq, inv, segments, start_date, start_time, start_ids):\n",
    "    \"\"\"\n",
    "    This function preprocesses the EEG input data, extracts connectivity matrices for AEC, ImCoh, PLI and wPLI,\n",
    "    and also features like power, regional-averaged connectivity values, and graph theory metrics.\n",
    "    It also applies filtering based on specified frequency range and checks for artifacts in the EEG signal. \n",
    "\n",
    "    Args:\n",
    "    data (np.array): The EEG data to be preprocessed (Channels x Timepoints).\n",
    "    filename (str): The name of the file being processed.\n",
    "    stage (int): Stage of the sleep-wake cycle being considered.\n",
    "    fromfreq (float): Lower frequency bound for filtering.\n",
    "    tofreq (float): Upper frequency bound for filtering.\n",
    "    inv (bool): If True, data inversion will be performed. If False, no data inversion.\n",
    "    segments (pandas.DataFrame): Dataframe containing the start and end times for the Awake segments.\n",
    "    start_date: Start date of the sleep annotations if needed.\n",
    "    start_time: Start time of the sleep annotations if needed.\n",
    "    start_ids: Start IDs of the sleep epochs being considered.\n",
    "    \n",
    "    Returns:\n",
    "    features (list): A list of dictionaries, each containing extracted features for the processed data.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    n_samples = data.shape[1]\n",
    "    \n",
    "    # Create bipolar channels\n",
    "    eeg_bipolar = {}\n",
    "    eeg_bipolar['chann'] = to_bipolar(data, CHANNEL_MAP)\n",
    "\n",
    "    # Clear unused variable from memory\n",
    "    del data\n",
    "    \n",
    "    ADEX_ID = filename[-7:-4]\n",
    "\n",
    "    if stage == 4:  # If Awake stage\n",
    "        # Use the start and end times of the Awake state segments instead of the imported start_ids\n",
    "        start = segments[segments['ID']==filename[-12:-4]]['Start'].iloc[0]\n",
    "        fin = segments[segments['ID']==filename[-12:-4]]['Fin'].iloc[0]\n",
    "        step = 30*FS\n",
    "        start_ids = np.arange(0, eeg_bipolar['chann'][0][:,:].shape[1], step)\n",
    "        start_ids = start_ids[(start_ids >= start) & (start_ids <= fin-step)]\n",
    "        # ADL: changed to (start_ids <= fin-step), instead of <= fin\n",
    "        # so that when you add the 30 secs to the last epoch, it is still within the marked wake period\n",
    "        \n",
    "    #  Remove epochs that would surpass data limit (data is segmented into 30s epochs in the next step) \n",
    "    start_ids = start_ids[~((start_ids + 30*FS) >= n_samples)] \n",
    "    \n",
    "    # Create an EEG without drift (for ARTIFACT detection) and segment the file \n",
    "    # ADL: changed dataeegf to include low freq filtering (for ARTIFACT detection)\n",
    "    dataeegf = mne.filter.filter_data(eeg_bipolar['chann'][0][:,:], FS, 0.5, 70, verbose=False, n_jobs = -1)\n",
    "    dataeegf = dataeegf[:,list(map(lambda x:np.arange(x,x+EPOCH_SIZE_S), start_ids))].transpose(1,0,2)\n",
    "    \n",
    "    # ADL: moved up: Filter signal in freq band of interest and format for MNE (for CONNECTIVITY)\n",
    "    dataeegffilt = mne.filter.filter_data(eeg_bipolar['chann'][0][:,:], FS, fromfreq, tofreq,verbose=False, n_jobs = -1) \n",
    "    dataeegffilt = dataeegffilt[:,list(map(lambda x:np.arange(x,x+EPOCH_SIZE_S), start_ids))].transpose(1,0,2)\n",
    "    \n",
    "    # ADL: modifed below to handle excluded channels\n",
    "    excl_chann_indices = []\n",
    "    chans_to_print = np.array(channels)\n",
    "        \n",
    "    if stage == 4 and 'Excl_chann' in segments.columns:  # for Awake state only\n",
    "        \n",
    "        # ADL added:  Initialize excluded channels during Awake to always include Fp1, Fp2\n",
    "        excl_chann_indices.extend([0,1,2])\n",
    "        \n",
    "        # Get the other excluded channels (for awake analysis) for this subject\n",
    "        excl_chann = segments[segments['ID'] == filename[-12:-4]]['Excl_chann'].dropna().tolist()\n",
    "        if excl_chann:\n",
    "            # Loop over the channels to exclude\n",
    "            for chan in excl_chann:\n",
    "                # Look for the channel in the defined channel list\n",
    "                for i, ch in enumerate(channels):\n",
    "                    # If the channel to exclude is in the defined channel, add to the list\n",
    "                    if chan in ch:\n",
    "                        excl_chann_indices.append(i)\n",
    "            \n",
    "            # ADL: added to print excluded channels\n",
    "            excl_chans_to_print = np.array(excl_chann_indices)\n",
    "            print('Channels to exclude: ' + str(chans_to_print[excl_chans_to_print]))\n",
    "       \n",
    "    # Replace the excluded channels in dataeegf with a timeseries of zeros   \n",
    "    if excl_chann_indices:\n",
    "        for i in excl_chann_indices:\n",
    "            dataeegf[:, i, :] = 0  # ADL: dataeegf is used only for artifact detection, so zeroing here is ok\n",
    "    \n",
    "    # Remove artifactual epochs on the unfiltered signal\n",
    "    SatAmp=400\n",
    "    EMGcutoff=3\n",
    "    LVcutoff=0.01\n",
    "    eyeblinkthr=200\n",
    "    eyeblink_mindiff = 20 # ADL added: Fp1, Fp2 peaks must be at least this close for eyeblink; 20 samps at 200Hz = 100ms  \n",
    "\n",
    "    # Check on 5s non-overlapping windows\n",
    "    wlen=5*FS\n",
    "\n",
    "    # Obtain myogenic signal\n",
    "    emg = np.zeros(dataeegf.shape)\n",
    "    vector = np.vectorize(np.float_)\n",
    "    emgfromfreq = 40\n",
    "    emgtofreq = 60\n",
    "    emg = mne.filter.filter_data(vector(dataeegf), 200, emgfromfreq, emgtofreq, verbose=False, n_jobs = -1)    \n",
    "    mean_std_emg = np.std(emg, axis = (0,2))\n",
    "\n",
    "    # Initialize variable to keep clean epochs (then we will trim the zeroes of the removed epochs)\n",
    "    dataeegclean = np.zeros(dataeegf.shape)\n",
    "\n",
    "    # Initialize clean epoch counter\n",
    "    c = 0\n",
    "\n",
    "    # Loop across epochs\n",
    "    for n in range(dataeegf.shape[0]):\n",
    "\n",
    "        # Initialize artifact count\n",
    "        nsatampepo = 0\n",
    "        nemgepo  = 0\n",
    "        nlvepo  = 0\n",
    "        neyeblinksepo = 0\n",
    "\n",
    "        # Loop across 5s non-overlapping windows\n",
    "        for t in range(0,dataeegf.shape[2],wlen):\n",
    "\n",
    "            blockEEG = dataeegf[n,:,t:t+wlen]  # nth epoch; all channels; samps for the 5 window\n",
    "            blockEMG = emg[n,:,t:t+wlen]\n",
    "            satEEG = np.max(np.abs(blockEEG), axis = 1)\n",
    "            stdEMG = np.std(blockEMG, axis = 1)\n",
    "            stdEEG = np.std(blockEEG, axis = 1)\n",
    "\n",
    "            # Check if F7-Fp1 and F8-Fp2 channels have peaks excededing 200uV \n",
    "            # Setting the prominence value to eyeblinkthr (i.e., 200 millivolts)\n",
    "            # Width 20 samples (100 ms at 200 Hz)\n",
    "            # ADL: changed distance to 50 samps (0.25sec at 200Hz); was previously 600 samples (3 seconds)\n",
    "            peaks1, properties1 = find_peaks(np.abs(blockEEG[1,:])-np.mean(blockEEG[1,:]), prominence=eyeblinkthr, width=20, distance=50)\n",
    "            peaks2, properties2 = find_peaks(np.abs(blockEEG[2,:])-np.mean(blockEEG[2,:]), prominence=eyeblinkthr, width=20, distance=50)\n",
    "\n",
    "            # Count number of artifacts in 5 sec windows in this 30s epoch\n",
    "            if (satEEG > SatAmp).any():\n",
    "                nsatampepo = nsatampepo + 1\n",
    "\n",
    "            if (stdEMG>EMGcutoff*mean_std_emg).any():\n",
    "                nemgepo = nemgepo + 1\n",
    "                    \n",
    "            # Exclude the removed channels (set to zero) from low voltage artifact check if any\n",
    "            if excl_chann_indices:\n",
    "                stdEEG_non_excluded = np.delete(stdEEG, np.s_[excl_chann_indices]) # ADL: changed for multiple chans\n",
    "            else:\n",
    "                stdEEG_non_excluded = stdEEG\n",
    "\n",
    "            if (stdEEG_non_excluded<LVcutoff).any():\n",
    "                nlvepo = nlvepo + 1\n",
    "\n",
    "            # ADL: modified below to check timing of eyeblink peaks\n",
    "            if peaks1.any():\n",
    "                if peaks2.any():\n",
    "                    smallest_diff = wlen\n",
    "                    # check to see if the Fp1,Fp2 peaks are close enough to be considered eyeblinks\n",
    "                    for num1 in peaks1:\n",
    "                        for num2 in peaks2:\n",
    "                            peakdiff = abs(num1 - num2)\n",
    "                            smallest_diff = min(smallest_diff, peakdiff)\n",
    "                    if smallest_diff <= eyeblink_mindiff:\n",
    "                        neyeblinksepo = neyeblinksepo + 1\n",
    "            \n",
    "        # If any artifact is present in this epoch, continue with the next epoch,\n",
    "        # otherwise, add the epoch to the clean output      \n",
    "        \n",
    "        # For asleep:  use all artifacts (saturation, emg, low voltage, eyeblinks)\n",
    "        if stage < 4:\n",
    "            if (nsatampepo + nemgepo + nlvepo + neyeblinksepo)> 0:\n",
    "                continue\n",
    "                \n",
    "        # For awake: use saturation, low voltage (no eyeblinks or EMG bc Fp1/Fp2 excluded, and not analyzing beta/gamma)\n",
    "        else:\n",
    "            if (nsatampepo + nlvepo)> 0:\n",
    "                continue            \n",
    "\n",
    "        # Adds clean freq-band filtered data to the output            \n",
    "        dataeegclean[c,:,:] = dataeegffilt[n,:,:]\n",
    "        c = c + 1\n",
    "\n",
    "    # Get the epochs without artifacts\n",
    "    out = dataeegclean[:c,:,:]\n",
    "    \n",
    "    perc_clean = (c/dataeegf.shape[0]) * 100\n",
    "    print('Percentage clean epochs = ' + str(perc_clean))\n",
    "    \n",
    "    if out.shape[0] == 0:\n",
    "        print('No clean epochs')\n",
    "        return None\n",
    "    \n",
    "    # Connectivity and other measurements below\n",
    "    # Calculate envelope correlation\n",
    "    envcor = envelope_correlation(out)\n",
    "    # Average through epochs\n",
    "    envcor = np.mean(envcor.get_data(), axis=0)\n",
    "    # ADL: modified below to set to NaN, rather than 0's\n",
    "    # For each excluded channel, set corresponding rows and columns in envcor to NAN \n",
    "    for idx in excl_chann_indices:\n",
    "        envcor[idx, :] = np.nan\n",
    "        envcor[:, idx] = np.nan\n",
    "        \n",
    "    # Calculate Imcoh (averaged through epochs)\n",
    "    imcoh = spectral_connectivity_epochs(out, sfreq=FS, fmin = fromfreq, fmax = tofreq, method='imcoh', faverage=True, verbose=0)\n",
    "    # Convert to symmetric matrix form\n",
    "    imcoh = imcoh.get_data().reshape(18,18)+imcoh.get_data().reshape(18,18).T\n",
    "    # Take care of excluded channels\n",
    "    # ADL: modified below to set to NaN, rather than 0's\n",
    "    for idx in excl_chann_indices:\n",
    "        imcoh[idx, :] = np.nan\n",
    "        imcoh[:, idx] = np.nan\n",
    "        \n",
    "    # Calculate PLI (averaged through epochs)\n",
    "    pli = spectral_connectivity_epochs(out, sfreq=FS, fmin = fromfreq, fmax = tofreq, method='pli', faverage=True, verbose=0)\n",
    "    # Convert to symmetric matrix form\n",
    "    pli = pli.get_data().reshape(18,18)+pli.get_data().reshape(18,18).T\n",
    "    # Take care of excluded channels\n",
    "    # ADL: modified below to set to NaN, rather than 0's\n",
    "    for idx in excl_chann_indices:\n",
    "        pli[idx, :] = np.nan\n",
    "        pli[:, idx] = np.nan\n",
    "        \n",
    "    # Calculate wPLI (averaged through epochs)\n",
    "    wpli = spectral_connectivity_epochs(out, sfreq=FS, fmin = fromfreq, fmax = tofreq, method='wpli', faverage=True, verbose=0)\n",
    "    # Convert to symmetric matrix form\n",
    "    wpli = wpli.get_data().reshape(18,18)+wpli.get_data().reshape(18,18).T\n",
    "    # Take care of excluded channels\n",
    "    # ADL: modified below to set to NaN, rather than 0's\n",
    "    for idx in excl_chann_indices:\n",
    "        wpli[idx, :] = np.nan\n",
    "        wpli[:, idx] = np.nan\n",
    "\n",
    "    # Calculate graph theory metrics\n",
    "    # ADL modified: these functions can't handle NaNs, but we want to exclude bad channels from these measurements\n",
    "    # ADL: create different matrices that don't include excluded channels\n",
    "    envcor_for_GT = envcor\n",
    "    if excl_chann_indices:\n",
    "        envcor_for_GT = np.delete(envcor_for_GT, np.s_[excl_chann_indices], axis=0)  # Delete rows \n",
    "        envcor_for_GT = np.delete(envcor_for_GT, np.s_[excl_chann_indices], axis=1)  # Delete columns\n",
    "\n",
    "    imcoh_for_GT = imcoh\n",
    "    if excl_chann_indices:\n",
    "        imcoh_for_GT = np.delete(imcoh_for_GT, np.s_[excl_chann_indices], axis=0)  # Delete rows \n",
    "        imcoh_for_GT = np.delete(imcoh_for_GT, np.s_[excl_chann_indices], axis=1)  # Delete columns\n",
    "\n",
    "    pli_for_GT = pli\n",
    "    if excl_chann_indices:\n",
    "        pli_for_GT = np.delete(pli_for_GT, np.s_[excl_chann_indices], axis=0)  # Delete rows \n",
    "        pli_for_GT = np.delete(pli_for_GT, np.s_[excl_chann_indices], axis=1)  # Delete columns\n",
    "\n",
    "    wpli_for_GT = wpli\n",
    "    if excl_chann_indices:\n",
    "        wpli_for_GT = np.delete(wpli_for_GT, np.s_[excl_chann_indices], axis=0)  # Delete rows \n",
    "        wpli_for_GT = np.delete(wpli_for_GT, np.s_[excl_chann_indices], axis=1)  # Delete columns\n",
    "        \n",
    "    ge_aecc = efficiency_wei(envcor_for_GT[:,:,0])       \n",
    "    ge_imcoh = efficiency_wei(imcoh_for_GT)\n",
    "    ge_pli = efficiency_wei(pli_for_GT)\n",
    "    ge_wpli = efficiency_wei(wpli_for_GT)\n",
    "    \n",
    "    sw_aecc = smallworldness_wei(envcor_for_GT[:,:,0])\n",
    "    sw_imcoh = smallworldness_wei(imcoh_for_GT)\n",
    "    sw_pli = smallworldness_wei(pli_for_GT)\n",
    "    sw_wpli = smallworldness_wei(wpli_for_GT)   \n",
    "        \n",
    "    # ADL added: Deal with excluded channels on PSD by removing these channels from the input to PSD\n",
    "    # Initiate input channels for each region\n",
    "    psd_front_chans = np.arange(7, dtype=int)\n",
    "    psd_post_chans = np.arange(13,17, dtype=int)\n",
    "    psd_postemp_chans = np.arange(9,17, dtype=int)\n",
    "    psd_frontemp_chans = np.arange(13, dtype=int)\n",
    "\n",
    "    if excl_chann_indices:  # remove excluded chans from the input channel list\n",
    "        psd_front_chans = np.setdiff1d(psd_front_chans, excl_chann_indices) \n",
    "        psd_post_chans = np.setdiff1d(psd_post_chans, excl_chann_indices)\n",
    "        psd_postemp_chans = np.setdiff1d(psd_postemp_chans, excl_chann_indices)\n",
    "        psd_frontemp_chans = np.setdiff1d(psd_frontemp_chans, excl_chann_indices)\n",
    "        \n",
    "    # Calculate power metrics and regional averaged FC for AEC-c, Imcoh, PLI, and wPLI\n",
    "    # ADL: changed np.mean to np.nanmean below\n",
    "    powerfront = psd(out[:,psd_front_chans,:], FS, fromfreq, tofreq)\n",
    "    aecfrontavg = np.nanmean(envcor[:7,:7,0])\n",
    "    imcfrontavg = np.nanmean(imcoh[:7,:7])\n",
    "    plifrontavg = np.nanmean(pli[:7,:7])\n",
    "    wplifrontavg = np.nanmean(wpli[:7,:7])\n",
    "    \n",
    "    powerpost = psd(out[:,psd_post_chans,:], FS, fromfreq, tofreq)\n",
    "    aecpostavg = np.nanmean(envcor[13:,13:,0])\n",
    "    imcpostavg = np.nanmean(imcoh[13:,13:])\n",
    "    plipostavg = np.nanmean(pli[13:,13:])\n",
    "    wplipostavg = np.nanmean(wpli[13:,13:])\n",
    "    \n",
    "    powerpostemp = psd(out[:,psd_postemp_chans,:], FS, fromfreq, tofreq)\n",
    "    aecpostempavg = np.nanmean(envcor[9:,9:,0])\n",
    "    imcpostempavg = np.nanmean(imcoh[9:,9:])\n",
    "    plipostempavg = np.nanmean(pli[9:,9:])\n",
    "    wplipostempavg = np.nanmean(wpli[9:,9:])\n",
    "    \n",
    "    powerfrontemp = psd(out[:,psd_frontemp_chans,:], FS, fromfreq, tofreq)\n",
    "    aecfrontempavg = np.nanmean(envcor[:13,:13,0])\n",
    "    imcfrontempavg = np.nanmean(imcoh[:13,:13])\n",
    "    plifrontempavg = np.nanmean(pli[:13,:13])\n",
    "    wplifrontempavg = np.nanmean(wpli[:13,:13])\n",
    "\n",
    "    # Append results for this subject in a list\n",
    "    features.append([str('ADEX_'+ADEX_ID), envcor[:,:,0],\n",
    "                  imcoh, pli, wpli,\n",
    "                  ge_aecc, sw_aecc,\n",
    "                  powerfront, aecfrontavg, imcfrontavg, plifrontavg, wplifrontavg,\n",
    "                  powerpost, aecpostavg, imcpostavg, plipostavg, wplipostavg,\n",
    "                  powerpostemp, aecpostempavg, imcpostempavg, plipostempavg, wplipostempavg,\n",
    "                  powerfrontemp, aecfrontempavg, imcfrontempavg, plifrontempavg, wplifrontempavg,\n",
    "                  perc_clean,\n",
    "                  ge_imcoh, ge_pli, ge_wpli,\n",
    "                  sw_imcoh, sw_pli, sw_wpli])\n",
    "\n",
    "    return features\n",
    "\n",
    "def merge_data(group1, gname1):\n",
    "    \"\"\"\n",
    "    Merges and processes feature data from multiple groups to create a pandas DataFrame and numpy arrays.\n",
    "\n",
    "    The function processes the features from each group individually and then combines them into a single \n",
    "    DataFrame. It also prepares numpy arrays for various metrics (AEC, imCoh, PLI, wPLI) of each group.\n",
    "\n",
    "    The inner function `process_group` is used to process each group individually. It extracts individual \n",
    "    features from each entry of the data, and stores them in separate lists.\n",
    "\n",
    "    Args:\n",
    "        group1 (list): A list containing features data for group 1. Each entry is a list containing IDs, metrics, and features., and features.\n",
    "        gname1 (str): The name for group 1. This will be used in the 'Class' column of the output DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        data_features (pd.DataFrame): A pandas DataFrame containing the merged features from all the input \n",
    "        groups. The columns are 'Class', 'ID', 'GE', 'SW', and the names of the other features.\n",
    "        metrics (dict): A dictionary where each key is a group name, and each value is another dictionary\n",
    "        that contains numpy arrays for various metrics (AEC, ImCoh, PLI, wPLI) of the respective group.\n",
    "    \"\"\"\n",
    "    def process_group(data):\n",
    "        data = [x for x in data if x is not None]\n",
    "        IDs, AEC, imcoh, pli, wpli, GE, SW = [], [], [], [], [], [], []\n",
    "\n",
    "        features = {feature: [] for feature in feature_names}\n",
    "\n",
    "        for entry in data:\n",
    "            IDs.append(entry[0][0])\n",
    "            AEC.append(entry[0][1])\n",
    "            imcoh.append(entry[0][2])\n",
    "            pli.append(entry[0][3])\n",
    "            wpli.append(entry[0][4])\n",
    "            GE.append(entry[0][5])\n",
    "            SW.append(entry[0][6])\n",
    "            for feature, value in zip(feature_names, entry[0][7:]):\n",
    "                features[feature].append(value)\n",
    "        \n",
    "        return IDs, AEC, imcoh, pli, wpli, GE, SW, features\n",
    "\n",
    "    feature_names = ['Power_Front', 'AEC_Front_avg', 'ImCoh_Front_avg', 'PLI_Front_avg', 'wPLI_Front_avg',\n",
    "                     'Power_Post', 'AEC_Post_avg', 'ImCoh_Post_avg', 'PLI_Post_avg', 'wPLI_Post_avg',\n",
    "                     'Power_Post_Temp', 'AEC_Post_Temp_avg', 'ImCoh_Post_Temp_avg', 'PLI_Post_Temp_avg', 'wPLI_Post_Temp_avg',\n",
    "                     'Power_Front_Temp', 'AEC_Front_Temp_avg', 'ImCoh_Front_Temp_avg', 'PLI_Front_Temp_avg', 'wPLI_Front_Temp_avg',\n",
    "                     'Percentage_clean_epochs', \n",
    "                     'GE_ImCoh', 'GE_PLI', 'GE_wPLI',\n",
    "                     'SW_ImCoh', 'SW_PLI', 'SW_wPLI']\n",
    "                    \n",
    "    groups = [group1]\n",
    "    group_names = [gname1]\n",
    "    metrics = {}\n",
    "    \n",
    "    for group, gname in zip(groups, group_names):\n",
    "        IDs, AEC, imcoh, pli, wpli, GE, SW, group_features = process_group(group)\n",
    "        metrics[gname] = {\"ID\": IDs, \"AEC\": AEC, \"ImCoh\": imcoh, \"PLI\": pli, \"wPLI\": wpli}\n",
    "        if gname == gname1:\n",
    "            d = dict(Class=[gname] * len(IDs), ID=IDs, GE_AEC=GE, SW_AEC=SW, **group_features)\n",
    "            data_features = pd.DataFrame(d)\n",
    "        else:\n",
    "            d = dict(Class=[gname] * len(IDs), ID=IDs, GE_AEC=GE, SW_AEC=SW, **group_features)\n",
    "            data_features = pd.concat([data_features, pd.DataFrame(d)], ignore_index=True)\n",
    "\n",
    "    return data_features, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_list(datapath, stages, freq_ranges, inv, segments, channels, second_night=False):\n",
    "    '''\n",
    "    This function processes and merges the input data for the specified datapath group, stages, and\n",
    "    frequency ranges. Now, it loads each file separately and preprocesses it for each stage and frequency\n",
    "    range combination before loading the next file.\n",
    "\n",
    "    Args:\n",
    "    datapath (str): The path to the data folder containing the group input files.\n",
    "    stages (list): A list of integers representing the stages to process (e.g. [1, 3, 4]).\n",
    "    freq_ranges (list): A list of dictionaries containing frequency range information, e.g.[{'name': 'delta', 'from': 0.5, 'to': 4}].\n",
    "    inv (bool): If True, data inversion will be performed. If False, no data inversion.\n",
    "    segments (pandas.DataFrame): Dataframe containing the start and end times for the Awake segments.\n",
    "    channels (list): Bipolar channel list.\n",
    "    second_night (bool): Analyze the time interval between 24hs and 48hs if recording exists\n",
    "\n",
    "    Returns:\n",
    "    results (dict): A dictionary with keys in the format \"{stage}_{freq_range['name']}\" and values as the processed and merged data for each stage and frequency range combination.\n",
    "    '''\n",
    "    \n",
    "    # Get the list of files for the specified group\n",
    "    files = glob.glob(datapath + '*.mat')\n",
    "    # Initialize an empty dictionary to store the results\n",
    "    results = {}\n",
    "\n",
    "    # Initialize results keys\n",
    "    for stage, freq_range in itertools.product(stages, freq_ranges):\n",
    "        key = f\"{stage}_{freq_range['name']}\"\n",
    "        results[key] = []\n",
    "\n",
    "    # Loop through all files\n",
    "    for filename in files:\n",
    "        print(filename)\n",
    "                \n",
    "        ## If the subject is in the excluded from entire analysis list, continue with the next subject\n",
    "        # ADL - why not just delete the pt from the folder?\n",
    "        if filename[-12:-4] in excl_entire:\n",
    "            print('Excluded from the entire analysis')\n",
    "            continue\n",
    "        \n",
    "        data = h5py.File(filename, 'r')\n",
    "        # convert each unicode code point to a character and join them into a string\n",
    "        string = ''.join(chr(c) for c in data['this_file_start'][:].flatten())\n",
    "        start_date = string.split()[0] \n",
    "        start_time = string.split()[1]\n",
    "\n",
    "        data = data['data'][:][:].T\n",
    "\n",
    "        \n",
    "        if filename[-12:-4] in inv:\n",
    "            \n",
    "            data = data*-1\n",
    "                \n",
    "        # Process staging files having different epoch starting times (i.e., not always starting 30s apart)\n",
    "        if filename[-12:-4] in stagingspecial:\n",
    "            print('Sleep stage epochs not always 30s apart (new method)')\n",
    "            ADEX_ID = filename[-7:-4]\n",
    "            filepath_sleep = './Staging/ssADEX_' + ADEX_ID + '.mat'\n",
    "\n",
    "            # Load staging file\n",
    "            spec_stag = sio.loadmat(filepath_sleep)['clean']\n",
    "            \n",
    "            # start_time should be a string of the form 'HH:MM:SS'\n",
    "            filestart_time = start_date + ' ' + start_time\n",
    "            # Check and apply the correct date format\n",
    "            if re.match(\"\\d{4}-\\d{2}-\\d{2}\", start_date):\n",
    "                filestart_time = datetime.strptime(filestart_time, '%Y-%m-%d %H:%M:%S')\n",
    "                date_format = '%Y-%m-%d %H:%M:%S'\n",
    "            elif re.match(\"\\d{2}-[A-Za-z]{3}-\\d{4}\", start_date):\n",
    "                filestart_time = datetime.strptime(filestart_time, '%d-%b-%Y %H:%M:%S')\n",
    "                date_format = '%d-%b-%Y %H:%M:%S'\n",
    "            else:\n",
    "                print(\"Unexpected date format in file: \", start_date)\n",
    "                return\n",
    "\n",
    "            # Initialize a new list for the corrected start_ids\n",
    "            start_ids_all_stages = []\n",
    "\n",
    "            # Initialize filtered_data\n",
    "            filtered_data = []\n",
    "\n",
    "            # Define time segment limits\n",
    "            first_day_end = filestart_time + timedelta(days=1)\n",
    "            start_cutoff = filestart_time + timedelta(days=1) if second_night else filestart_time\n",
    "            end_cutoff = filestart_time + timedelta(days=2) if second_night else first_day_end\n",
    "            \n",
    "            print('Original file start time: ' + str(filestart_time))\n",
    "            print('Start: ' + str(start_cutoff))\n",
    "            print('End: ' + str(end_cutoff))\n",
    "            \n",
    "            # Loop through spec_stag epochs\n",
    "            for entry in spec_stag:\n",
    "                entry_time_string = start_date + ' ' + entry[1][0]\n",
    "                entry_time = datetime.strptime(entry_time_string, date_format)\n",
    "                entry_day = entry[2][0][0]\n",
    "                entry_time += timedelta(days=int(entry_day - 1))\n",
    "\n",
    "                # If entry_time is within the desired interval\n",
    "                # extract the starting point of each epoch\n",
    "                if entry_time >= start_cutoff and entry_time < end_cutoff:\n",
    "                    filtered_data.append(entry)\n",
    "                    time_diff = (entry_time - filestart_time).total_seconds()\n",
    "                    start_id = time_diff * FS\n",
    "                    start_ids_all_stages.append(int(start_id))\n",
    "    \n",
    "            # Convert start_ids (timeseries list) and filtered_data (datenum format entries being considered)\n",
    "            # to numpy arrays\n",
    "            start_ids_all_stages = np.array(start_ids_all_stages)\n",
    "            filtered_data = np.array(filtered_data)\n",
    "            \n",
    "            if start_ids_all_stages.shape[0] == 0:\n",
    "            ## No timepoints available for 2nd night\n",
    "                print('No timepoints available for 2nd night')\n",
    "                continue\n",
    "                \n",
    "            # Print info to check timeseries points and datenum starting times\n",
    "            print('File start time: ' + str(start_cutoff))\n",
    "            print('First three time tags: ' + str([item[1][0] for item in filtered_data[:3]]))\n",
    "            print('First three timepoints: ' + str(start_ids_all_stages[:3]))\n",
    "            print('Last time tag of next day:  ' + str(filtered_data[-1][1][0]))\n",
    "            \n",
    "            # Subtract 1 to follow nomenclature and store extracted stages\n",
    "            stages_extracted = np.array([sub_arr[0][0][0] for sub_arr in filtered_data]) - 1\n",
    "\n",
    "            # Loop through all combinations of stages and frequency ranges\n",
    "            for stage, freq_range in itertools.product(stages, freq_ranges):\n",
    "                key = f\"{stage}_{freq_range['name']}\"\n",
    "                print(key)\n",
    "                \n",
    "                # Get the timeseries starting times of epochs of a particular stage\n",
    "                start_ids = start_ids_all_stages[stages_extracted == stage].astype(int)\n",
    "                if start_ids.shape[0] == 0:\n",
    "                    ## No sleep of the desired stage in this recording, continue with the next subject\n",
    "                    print('No sleep of that stage in this subject')\n",
    "                    continue\n",
    "\n",
    "                if stage == 4 and filename[-12:-4] in excl_awake:\n",
    "                    ## No sleep of the desired stage in this recording, continue with the next subject\n",
    "                    print('Excluded from awake analysis')\n",
    "                    continue\n",
    "                    \n",
    "                else:\n",
    "                    print(data.shape)\n",
    "                    result = preprocess(data, filename, stage=stage, fromfreq=freq_range['from'],\n",
    "                                        tofreq=freq_range['to'], inv=inv, segments = segments,\n",
    "                                        start_date = start_date, start_time = start_time, start_ids = start_ids)\n",
    "                    results[key].append(result)\n",
    "            \n",
    "        # Standard processing from old staging files (Maurice's approach)\n",
    "        else:\n",
    "            print('Staging file having epochs always 30s apart (Maurice method)')\n",
    "            # Get the time tags to segment EEG in epochs always starting 30 seconds apart (30s x sampling rate)\n",
    "            n_samples = data.shape[1]\n",
    "            start_ids_all_stages = np.arange(0, n_samples, EPOCH_SIZE_S)  #  Make array\n",
    "\n",
    "            # Load staging file \n",
    "            ADEX_ID = filename[-7:-4]\n",
    "            filepath_sleep = './Staging/ssADEX_' + ADEX_ID + '.mat'\n",
    "            sleep_stages = sio.loadmat(filepath_sleep)['labels'][0] - 1  # Follow sleep stage nomenclature\n",
    "\n",
    "            # Adjust based on second_night flag\n",
    "            if second_night:\n",
    "                firstnight_stages = 2*60*24\n",
    "                secondnight_stages = 2*60*48\n",
    "\n",
    "                # Check if data is available for the second night\n",
    "                if len(sleep_stages) < secondnight_stages:\n",
    "                    print('Staging not long enough to get 2nd night data')\n",
    "                    continue\n",
    "\n",
    "                # Get the staging file epochs from 24 hours to 48 hours\n",
    "                sleep_stages = sleep_stages[firstnight_stages:secondnight_stages]\n",
    "                # Truncate 30 second timepoint array to up to 24 hours\n",
    "                start_ids_all_stages = start_ids_all_stages[firstnight_stages:secondnight_stages]\n",
    "                print('First three timepoints: ' + str(start_ids_all_stages[:3]))\n",
    "            else:\n",
    "                # If not second night, simply use data for first 24 hours\n",
    "                # Consider the first 24hs 30s epochs to analyze first night (2 x 60 x 24)\n",
    "                firstnight_stages = 2*60*24\n",
    "                # Get the staging file epochs to up to 24 hours\n",
    "                sleep_stages = sleep_stages[:firstnight_stages]\n",
    "                # Truncate 30 second timepoint array to up to 24 hours\n",
    "                start_ids_all_stages = start_ids_all_stages[:len(sleep_stages)]\n",
    "                print('First three timepoints: ' + str(start_ids_all_stages[:3]))\n",
    "\n",
    "            # Initialize stage-specific start id\n",
    "            start_ids = []\n",
    "            for stage, freq_range in itertools.product(stages, freq_ranges):\n",
    "                key = f\"{stage}_{freq_range['name']}\"\n",
    "                print(key)\n",
    "                # Get the start_ids of the desired stage\n",
    "                start_ids = start_ids_all_stages[sleep_stages == stage]\n",
    "                \n",
    "                if start_ids.shape[0] == 0:\n",
    "                    ## No sleep of the desired stage in this recording, continue with the next subject\n",
    "                    print('No epochs for this sleep stage')\n",
    "                    continue\n",
    "                    \n",
    "                if stage == 4 and filename[-12:-4] in excl_awake:\n",
    "                    ## No sleep of the desired stage in this recording, continue with the next subject\n",
    "                    print('File excluded from awake analysis')\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "\n",
    "                    result = preprocess(data, filename, stage=stage, fromfreq=freq_range['from'],\n",
    "                                        tofreq=freq_range['to'], inv=inv, segments = segments,\n",
    "                                        start_date = start_date, start_time = start_time, start_ids = start_ids)\n",
    "                    results[key].append(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ziYm-kEoizBj",
    "outputId": "81c1131d-ec37-413d-f800-2c5b30009c94"
   },
   "outputs": [],
   "source": [
    "# Set the datapath\n",
    "# Note: The sleep staging files should be placed in a \"./Staging\" folder in the datapath\n",
    "\n",
    "datapath = './'\n",
    "\n",
    "# Define stages\n",
    "# \"1\" for N2, \"3\" for REM, and \"4\" for Awake state\n",
    "stages = [1, 3, 4]\n",
    "\n",
    "# Load the selected epochs to employ awake state segments\n",
    "awake_segments = pd.read_csv(datapath + 'awake_segments_v2.csv')\n",
    "\n",
    "# Define frequency bands\n",
    "freq_ranges = [{'name': 'delta', 'from': 0.5, 'to': 4},\n",
    "               {'name': 'theta', 'from': 4, 'to': 8},\n",
    "               {'name': 'alpha', 'from': 8, 'to': 12},\n",
    "               {'name': 'beta', 'from': 12, 'to': 30},  \n",
    "               {'name': 'gamma', 'from': 30, 'to': 50}]\n",
    "\n",
    "group_result = {}\n",
    "\n",
    "## Besides having the Staging folder in the datapath, they should be another called ./Second_Night\n",
    "## for the extended files\n",
    "\n",
    "for path in ([datapath + 'Second_Night/']):\n",
    "\n",
    "    if path not in group_result:\n",
    "        group_result[path] = {}\n",
    "\n",
    "    results = process_file_list(path, stages, freq_ranges, invert_files, awake_segments, channels,\n",
    "                                second_night=True)\n",
    "\n",
    "    for key, result in results.items():\n",
    "        if key not in group_result[path]:\n",
    "            group_result[path][key] = []\n",
    "        group_result[path][key].append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through group results to build output data structures, including Pandas tables \n",
    "# for later Graph Theory and Machine Learning analysis\n",
    "features, metrics = {}, {}\n",
    "\n",
    "for stage, freq_range in itertools.product(stages, freq_ranges):\n",
    "    key = f\"{stage}_{freq_range['name']}\"\n",
    "    features[key], metrics[key] = merge_data(group_result[datapath + 'Second_Night/'][key][0],\n",
    "                                             'Second_Night')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save features\n",
    "def save_data(data, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "# Save the data structures to files\n",
    "save_data(group_result, 'group_result_2nd_night.pickle')\n",
    "save_data(features, 'features_2nd_night.pickle')\n",
    "save_data(metrics, 'metrics_2nd_night.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1QA931CwYcSpSd36smVaewB-uex3b7qQK",
     "timestamp": 1684441108863
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
